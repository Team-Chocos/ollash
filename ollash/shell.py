# ollash/shell.py
import subprocess
import sys
import os
import time
import threading
from ollash.utils import ensure_ollama_ready, is_model_installed, pull_model_with_progress, get_os_label
from ollash.menu_advanced import get_model_selection_advanced

# Try to import readline for better input editing
try:
    import readline
    HAS_READLINE = True
except ImportError:
    HAS_READLINE = False


class ThinkingAnimation:
    """Animated thinking indicator"""
    def __init__(self, message="Thinking"):
        self.message = message
        self.running = False
        self.thread = None
        self.frames = ["‚†ã", "‚†ô", "‚†π", "‚†∏", "‚†º", "‚†¥", "‚†¶", "‚†ß", "‚†á", "‚†è"]
        self.current_frame = 0

    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._animate)
        self.thread.daemon = True
        self.thread.start()

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join()
        # Clear the line
        print(f"\r{' ' * (len(self.message) + 10)}", end='\r')

    def _animate(self):
        while self.running:
            frame = self.frames[self.current_frame]
            print(f"\r{frame} {self.message}...", end='', flush=True)
            self.current_frame = (self.current_frame + 1) % len(self.frames)
            time.sleep(0.1)


def input_with_prefill(prompt, prefill=''):
    """Input function that prefills the input with given text"""
    if HAS_READLINE and prefill:
        def startup_hook():
            readline.insert_text(prefill)
        readline.set_startup_hook(startup_hook)
        try:
            return input(prompt)
        finally:
            readline.set_startup_hook(None)
    else:
        # Fallback for systems without readline - just use regular input
        return input(prompt)


def animate_arrow():
    """Animate arrow appearance"""
    arrow_frames = ["", "‚îÄ", "‚îÄ>", "‚îÄ‚Üí"]
    for frame in arrow_frames:
        print(f"\r‚îÇ \033[36m{frame}\033[0m", end='', flush=True)
        time.sleep(0.1)
    print()


def animate_lightning():
    """Animate lightning bolt"""
    lightning_frames = ["‚ö¨", "‚ö°", "‚ö°"]
    colors = ["\033[33m", "\033[93m", "\033[33m"]  # yellow variations
    for i, (frame, color) in enumerate(zip(lightning_frames, colors)):
        print(f"\r‚îÇ {color}{frame}\033[0m", end='', flush=True)
        time.sleep(0.15)
    print()


def animate_success():
    """Animate success checkmark"""
    success_frames = ["‚óã", "‚óê", "‚óë", "‚óí", "‚óì", "‚óè", "‚úì"]
    for frame in success_frames:
        if frame == "‚úì":
            print(f"\r‚îÇ \033[32m{frame}\033[0m Command completed successfully", flush=True)
        else:
            print(f"\r‚îÇ \033[32m{frame}\033[0m", end='', flush=True)
        time.sleep(0.1)


def animate_failure():
    """Animate failure X mark"""
    failure_frames = ["‚óã", "‚óë", "‚óí", "‚óì", "‚óè", "‚úó"]
    for frame in failure_frames:
        if frame == "‚úó":
            print(f"\r‚îÇ \033[31m{frame}\033[0m Command failed", flush=True)
        else:
            print(f"\r‚îÇ \033[31m{frame}\033[0m", end='', flush=True)
        time.sleep(0.1)


def get_command_suggestion(prompt: str, model: str) -> str:
    """Get command suggestion from Ollama without interactive prompts"""
    if not is_model_installed(model):
        pull_model_with_progress(model)

    os_label = get_os_label()
    
    ollama_cmd = [
        "ollama", "run", model,
        f"Translate the following instruction into a safe {os_label} terminal command. Respond ONLY with the command, no explanation:\nInstruction: {prompt}"
    ]

    try:
        response = subprocess.run(
            ollama_cmd,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore"
        )

        raw_output = response.stdout.strip()
        
        # Extract command
        import re
        match = re.search(r"`([^`]+)`", raw_output)
        command = match.group(1).strip() if match else raw_output.strip().splitlines()[0]
        
        return command
    except Exception as e:
        raise Exception(f"Failed to get command suggestion: {e}")


def execute_command(command: str) -> bool:
    """Execute a command and return True if successful"""
    try:
        result = subprocess.run(command, shell=True)
        return result.returncode == 0
    except KeyboardInterrupt:
        print("\n‚îÇ [Interrupted]")
        return False
    except Exception as e:
        print(f"‚îÇ Error: {e}")
        return False


def clear_screen():
    """Clear the terminal screen"""
    os.system('cls' if os.name == 'nt' else 'clear')


def print_box_line(content="", width=70, style="middle"):
    """Print a line with box styling"""
    if style == "top":
        print(f"‚îå{'‚îÄ' * (width-2)}‚îê")
    elif style == "bottom":
        print(f"‚îî{'‚îÄ' * (width-2)}‚îò")
    elif style == "separator":
        print(f"‚îú{'‚îÄ' * (width-2)}‚î§")
    elif style == "middle":
        padding = width - len(content) - 4
        left_pad = padding // 2
        right_pad = padding - left_pad
        print(f"‚îÇ {' ' * left_pad}{content}{' ' * right_pad} ‚îÇ")
    elif style == "left":
        padding = width - len(content) - 4
        print(f"‚îÇ {content}{' ' * padding} ‚îÇ")


def print_banner(model):
    """Print beautiful boxed banner"""
    width = 70
    print_box_line(style="top", width=width)
    print_box_line("", width=width)
    print_box_line("OLLASH SHELL", width=width)
    print_box_line(f"Model: {model}", width=width)
    print_box_line("", width=width)
    print_box_line(style="bottom", width=width)


def print_help():
    """Print help information in a box"""
    width = 70
    print()
    print_box_line(style="top", width=width)
    print_box_line("COMMANDS", width=width)
    print_box_line(style="separator", width=width)
    print_box_line("<natural language>    Get command suggestion", width=width, style="left")
    print_box_line("", width=width)
    print_box_line("Special Commands:", width=width, style="left")
    print_box_line(":help                Show this help", width=width, style="left")
    print_box_line(":clear               Clear screen", width=width, style="left")
    print_box_line(":model <name>        Switch model", width=width, style="left")
    print_box_line(":exit, :quit         Exit shell", width=width, style="left")
    print_box_line("", width=width)
    print_box_line("Shortcuts:", width=width, style="left")
    print_box_line("Ctrl+C               Cancel operation", width=width, style="left")
    print_box_line("Ctrl+D               Exit shell", width=width, style="left")
    print_box_line(style="bottom", width=width)
    print()


def format_prompt(model):
    """Create a clean prompt with box styling"""
    return f"‚îÇ [{model}] ‚ùØ "


def print_status(message, status_type="info", in_box=True):
    """Print formatted status messages"""
    symbols = {
        "info": "‚Ñπ",
        "success": "‚úì",
        "error": "‚úó",
        "suggestion": "‚Üí",
        "executing": "‚ö°"
    }
    symbol = symbols.get(status_type, "‚Ñπ")
    
    if in_box:
        print_box_line(f"{symbol} {message}", width=70, style="left")
    else:
        print(f"{symbol} {message}")


def print_suggested_command(command):
    """Print suggested command with clean aesthetic"""
    print()
    print(f"‚îÇ \033[36m‚Üí\033[0m \033[1m{command}\033[0m")
    print("‚îÇ")


def print_execution_start(command):
    """Print execution start with clean style"""
    print()
    print(f"‚îÇ \033[33m‚ö°\033[0m Executing: \033[90m{command}\033[0m")
    print("‚îÇ " + "‚îÄ" * 50)


def print_execution_result(success):
    """Print execution result with clean styling"""
    if success:
        print("‚îÇ " + "‚îÄ" * 50)
        print(f"‚îÇ \033[32m‚úì\033[0m Command completed successfully")
    else:
        print("‚îÇ " + "‚îÄ" * 50)
        print(f"‚îÇ \033[31m‚úó\033[0m Command failed")


def main(model=None, backend=None):
    """Main REPL shell function with interactive model selection"""
    
    # Interactive model selection if no model specified
    if not model:
        print("ü§ñ No model specified, starting interactive selection...")
        selection = get_model_selection_advanced(method="pyfzf")
        
        if not selection:
            print("‚ùå No model selected. Exiting...")
            return
        
        backend, model = selection
    else:
        # Default to ollama if backend not specified
        backend = backend or "ollama"
    
    print(f"\nüöÄ Starting Ollash with {model} on {backend}")
    
    # Initial setup based on backend
    try:
        if backend == "ollama":
            ensure_ollama_ready()
            if not is_model_installed(model):
                animation = ThinkingAnimation("Installing model")
                animation.start()
                pull_model_with_progress(model)
                animation.stop()
        elif backend == "llama-cpp":
            # Setup llama.cpp if needed
            setup_llamacpp(model)
            
    except Exception as e:
        print_status(f"Setup failed: {e}", "error", in_box=False)
        return

    # Welcome screen
    clear_screen()
    print_banner(f"{model} ({backend})")
    print()
    print_status("Ready! Type ':help' for commands", "success", in_box=False)
    print()

    while True:
        try:
            # Get user input
            try:
                user_input = input(format_prompt(model)).strip()
            except EOFError:
                print("\n‚îÇ Goodbye!")
                break
            except KeyboardInterrupt:
                print()
                continue

            if not user_input:
                continue

            # Handle special commands
            if user_input in [":exit", ":quit"]:
                print("‚îÇ Goodbye!")
                break
            
            elif user_input == ":help":
                print_help()
                continue
            
            elif user_input == ":clear":
                clear_screen()
                print_banner(model)
                print()
                continue
            
            elif user_input.startswith(":model "):
                new_model = user_input[7:].strip()
                if new_model:
                    if not is_model_installed(new_model):
                        try:
                            animation = ThinkingAnimation(f"Installing model '{new_model}'")
                            animation.start()
                            pull_model_with_progress(new_model)
                            animation.stop()
                            model = new_model
                            print_status(f"Switched to model: {model}", "success", in_box=False)
                        except Exception as e:
                            print_status(f"Failed to load model '{new_model}': {e}", "error", in_box=False)
                    else:
                        model = new_model
                        print_status(f"Switched to model: {model}", "success", in_box=False)
                else:
                    print_status("Please specify a model name", "error", in_box=False)
                continue
            elif user_input.startswith(":sh "):
                command = user_input[4:]
                print_execution_start(command)
                success = execute_command(command)
                print_execution_result(success)
                continue

            # Get command suggestion
            try:
                animation = ThinkingAnimation("Generating command")
                animation.start()
                command = get_command_suggestion(user_input, model)
                animation.stop()
                
                print_suggested_command(command)
                
                # Ask if user wants to run it
                while True:
                    try:
                        choice = input("‚îÇ Execute? [y/N/e(dit)] ‚ùØ ").strip().lower()
                        if choice in ['', 'n', 'no']:
                            break
                        elif choice in ['y', 'yes']:
                            print_execution_start(command)
                            success = execute_command(command)
                            print_execution_result(success)
                            break
                        elif choice in ['e', 'edit']:
                            try:
                                edited_command = input_with_prefill("‚îÇ Edit ‚ùØ ", command).strip()
                                if edited_command:
                                    command = edited_command
                                    print_execution_start(command)
                                    success = execute_command(command)
                                    print_execution_result(success)
                                break
                            except (EOFError, KeyboardInterrupt):
                                print("\n‚îÇ Cancelled")
                                break
                        else:
                            print("‚îÇ Enter 'y' (yes), 'n' (no), or 'e' (edit)")
                    except (EOFError, KeyboardInterrupt):
                        print("\n‚îÇ Cancelled")
                        break
                        
            except Exception as e:
                print_status(f"Error: {e}", "error", in_box=False)
                
            print()  # Add spacing between operations
                
        except KeyboardInterrupt:
            print()
            continue
        except Exception as e:
            print_status(f"Unexpected error: {e}", "error", in_box=False)
            continue

    # Cleanup
    try:
        animation = ThinkingAnimation(f"Stopping model: {model}")
        animation.start()
        subprocess.run(["ollama", "stop", model], capture_output=True)
        animation.stop()
        print(f"‚îÇ Model stopped")
    except:
        pass

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Run Ollash REPL Shell"
    )
    parser.add_argument(
        "--model", type=str, help="Model name to use (e.g., llama3:8b)"
    )
    parser.add_argument(
        "--backend", type=str, choices=["ollama", "llama-cpp"], help="Backend to use"
    )

    args = parser.parse_args()

    # Run the shell with optional model/backend
    main(model=args.model, backend=args.backend)
